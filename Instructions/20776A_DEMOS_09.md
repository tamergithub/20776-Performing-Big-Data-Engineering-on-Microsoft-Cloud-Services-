# Module 9: Automating Data Flow with Azure Data Factory

- [Module 9: Automating Data Flow with Azure Data Factory](#module-9-automating-data-flow-with-azure-data-factory)
    - [Demo 1: Creating and running an Azure Data Factory pipeline](#demo-1-creating-and-running-an-azure-data-factory-pipeline)
        - [Scenario](#scenario)
        - [Prepare a local database for use with Data Factory](#prepare-a-local-database-for-use-with-data-factory)
        - [Create a new Data Factory](#create-a-new-data-factory)
        - [Create a Data Management Gateway](#create-a-data-management-gateway)
        - [Create a Data Factory pipeline with a copy activity to move the data](#create-a-data-factory-pipeline-with-a-copy-activity-to-move-the-data)
        - [Verify the Data Factory pipeline](#verify-the-data-factory-pipeline)
    - [Demo 2: Creating a pipeline using the Data Factory Copy Wizard](#demo-2-creating-a-pipeline-using-the-data-factory-copy-wizard)
        - [Scenario](#scenario)
        - [Create a copy data activity and pipeline using the wizard](#create-a-copy-data-activity-and-pipeline-using-the-wizard)
        - [Verify and test the new pipeline](#verify-and-test-the-new-pipeline)
    - [Demo 3: Using Machine Learning in an Azure Data Factory pipeline](#demo-3-using-machine-learning-in-an-azure-data-factory-pipeline)
        - [Scenario](#scenario)
        - [Create and deploy an ML model ready for use with Data Factory](#create-and-deploy-an-ml-model-ready-for-use-with-data-factory)
        - [Upload live data as a test dataset](#upload-live-data-as-a-test-dataset)
        - [Create a Data Factory Machine Learning linked service](#create-a-data-factory-machine-learning-linked-service)
        - [Create Data Factory input and output datasets](#create-data-factory-input-and-output-datasets)
        - [Create a new Data Factory pipeline](#create-a-new-data-factory-pipeline)
        - [Verify and test the ML pipeline](#verify-and-test-the-ml-pipeline)
    - [Demo 4: Demonstration: Using the Monitoring and Management app](#demo-4-demonstration-using-the-monitoring-and-management-app)
        - [Scenario](#scenario)
        - [Use the Diagram view](#use-the-diagram-view)
        - [Use filters](#use-filters)
        - [Pause and resume a pipeline](#pause-and-resume-a-pipeline)
        - [Use monitoring views](#use-monitoring-views)
        - [Use alerts](#use-alerts)
  
## Demo 1: Creating and running an Azure Data Factory pipeline

### Scenario

In this demonstration, you will see how to:

- Prepare a local database for use with Data Factory.
- Create a new Data Factory.
- Create a Data Management Gateway.
- Create Data Factory linked services for source and sink data stores.
- Create Data Factory datasets to represent input and output data.
- Create a Data Factory pipeline with a copy activity to move the data.
- Verify the Data Factory pipeline.

### Prepare a local database for use with Data Factory

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to 20776A-LON-DEV as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. On the Windows **Start** menu, type **Microsoft SQL Server Management Studio**, and then press Enter.
3. In the **Connect to Server** dialog box, in the **Server name** box, type **LON-SQL**.
4. In the **Authentication** list, click **Windows Authentication**, and then click **Connect**.
5. In Object Explorer, expand **LON-SQL**, right-click **Databases**, and then click **New Database**.
6. In the **New Database** dialog box, in the **Database name** box, type **StockPrices**, and then click **OK**.
7. In Object Explorer, expand **Databases**, right-click **StockPrices**, and then click **New Query**.
8. In the SQL Editor, type the following commands, and then click **Execute**:

    ```SQL
    USE StockPrices
    GO
    CREATE TABLE StockPriceData
    (
        Ticker VARCHAR(4) NOT NULL,
        ClosingPrice VARCHAR(4) NOT NULL,
        OpeningPrice VARCHAR(4) NOT NULL,
        PriceChange VARCHAR(4) NOT NULL,
        PercentChange VARCHAR(30) NOT NULL,
        HourOfLastTrade VARCHAR(2) NOT NULL,
        Suspicious VARCHAR(3) NOT NULL
    )
    GO
    ```

    You can copy these commands from the file **E:\\Demofiles\\Mod09\\Demo1\\SqlCmd1.txt**

9. On the Windows **Start** menu, type **command prompt**, and then press Enter.
10. At the command prompt, run the following commands:

    ```CMD
    E:
    cd E:\Demofiles\Mod09\Demo1
    bcp StockPrices.dbo.StockPriceData in StockPriceData.csv /T /SLON-SQL /c /t,
    ```

    These commands upload sample stock price data into the **StockPricesData** table. It should read in approxiamtely 46,400 rows.

    > **Note**: Ensure that you include the comma at the end of the lst line.

    You copy these commands from the file **E:\\Demofiles\\Mod09\\Demo1\\BcpCmd1.txt**.

11. Return to Microsoft SQL Server Management Studio.
12. In the SQL Editor, replace the existing code with the following statement, and then click **Execute**:

    ```SQL
    SELECT TOP(2000) *
    FROM StockPriceData
    GO
    ```

    You can copy this statement from the file **E:\Demofiles\Mod09\Demo1\SqlCmd2.txt**. 

    This command should display the first 2,000 rows of data; point out that the uploaded data contains stock ticker names, opening and closing prices, the price change (absolute and percentage) during the trading period, the hour of day of the last trade, and whether the trade has been marked as suspicious. As you scroll down the data, point out that the PercentChange column contains integers, decimal numbers, and strings.

13. Close Microsoft SQL Server Management Studio without saving any changes.

### Create a new Data Factory

1. Switch to the Azure portal.
2. Click **+ Create a resource**, **Analytics**, and then click **Data Factory**.
3. On the **New data factory** blade, in the **Name** box, type **stocksdf&lt;your name&gt;&lt;date&gt;**.
4. Under **Resource group**, click **Create new**, and then type **StocksDF-RG**.
5. In the **Version** list, click **V1**.
6. In the **Location** list, select your nearest location from the currently available Data Factory regions,and then click **Create**.
7. Wait until the data factory has deployed before continuing with the demo.

### Create a Data Management Gateway

1. In the Azure portal, click **All resources**, and then click **stocksdf&lt;your name&gt;&lt;date&gt;**.
2. If the **Settings** blade opens, close it.
3. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, in the **Actions** section, click **Author and deploy**.
4. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **More**, and then click **New integration runtime (gateway)**.
5. On the **Create** blade, in the **Name** box, type **StocksGWY**, and then click **OK**.
6. On the **Configure** blade, click **Install directly on this computer**.
7. In the Application Run - Security Warning dialog box, click Run.
8. In the User Account Control dialog box, click Yes.
9. In the Microsoft Integration Runtime Express Setup dialog box, when the installation has completed, click Close.
10. If the Message from webpage dialog box appears, click OK.
11. On the Start menu, type Microsoft Integration Runtime, and then press Enter.
12. In the User Account Control dialog box, click Yes.
13. In Microsoft Integration Runtime Configuration Manager, point out that this node is connected to the cloud service.
14. Click Settings, and point out that the endpoint is using a self-signed certificate auto-generated by the Data Factory service.
15. Click Diagnostics, in the Test Connection section, enter the following details, and then click Test:
    - Data source type: SqlServer
    - Server: LON-SQL
    - Database: StockPrices
    - Authentication mode: Windows
    - User name: ADATUM\AdatumAdmin
    - Password: Pa55w.rd
16. Point out the green tick, which shows a successful test connection.
17. Switch to the Azure portal.
18. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, expand Integration runtimes (Gateways), and then click StocksGWY. Note that the JSON-formatted information is the configuration detail for the gateway. Create Data Factory linked services for source and sink data stores
19. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, click New data store, and then click SQL Server.
20. In the JSON editor, edit the following properties, and then click Deploy:
    - connectionString: replace the existing value with Data Source=LON-SQL;Initial Catalog=StockPrices;Integrated Security=True;
    - gatewayName: replace the existing value with StocksGWY
    - username: replace the existing value with ADATUM\\AdatumAdmin
    - password: replace the existing value with Pa55w.rd
21. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, expand Linked services, and then click SqlServerLinkedService. The JSON-formatted information is the configuration detail for the connection to the on-premises SQL Server, and show that the password has now been obfuscated with *********.
22. In the Azure portal, click + New, click Storage, and then click Storage account - blob, file, table,queue.
23. On the Create Storage account blade, in the Name box, type stockstore&lt;your name&gt;&lt;date&gt;.
24. Under Resource group, click Use existing, and then click StocksDF-RG.
25. In the Location list, select the same location as you used for the Data Factory.
26. Leave all other details at their defaults, and click Create.
27. Wait until the storage account has been successfully created before continuing with the demo.
28. Click All resources, and then click stockstore&lt;your name&gt;&lt;date&gt;.
29. On the stockstore&lt;your name&gt;&lt;date&gt; blade, under BLOB SERVICE, click Containers, and then click + Container.
30. In the New container dialog box, in the Name box, type pricedata, and then click OK.
31. On the stockstore&lt;your name&gt;&lt;date&gt; blade, under SETTINGS, click Access keys.
32. Next to key1, click the Click to copy button, to copy the key to the clipboard.
33. In the Azure portal, click All resources, and then click stocksdf&lt;your name&gt;&lt;date&gt;.
34. If the Settings blade opens, close it.
35. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, in the Actions section, click Author and deploy.
36. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, click New data store, and then click Azure Storage.
37. In the JSON editor, in connectionString, replace &lt;accountname&gt; with stockstore&lt;your name&gt;&lt;date&gt;, and replace &lt;accountkey&gt; with the storage access key you copied to the clipboard, and then click Deploy.
38. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, expand Linked services, and then click AzureStorageLinkedService. The JSON-formatted information is the configuration detail for the connection to the storage account, and show that the storage key has also been obfuscated with *********. Create Data Factory datasets to represent input and output data 1. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, click More, click New dataset, and then click SQL Server table.
39. Replace the existing JSON text with the following, and then click Deploy:

    ```JSON
    {
        "name": "Input SQL Server Dataset",
        "properties": {
            "type": "SqlServerTable",
            "linkedServiceName": "SqlServerLinkedService",
            "structure": [],
            "typeProperties": {
               "tableName": "StockPriceData"
            },
            "external": true,
            "availability": {
                "frequency": "Minute",
                "interval": "30"
            },
            "policy": {
                "externalData": {
                    "retryInterval": "00:01:00",
                    "retryTimeout": "00:10:00",
                    "maximumRetry": 3
                }  
            }
        }
    }
    ```

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo1\\JsonCmd1.txt**.

40. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, expand Datasets, and then click Input SQL Server Dataset. The JSON-formatted information is the configuration detail for the on-premises SQL Server dataset.
41. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, click More, click New dataset, and then click Azure Blob storage.
42. Replace the existing JSON text with the following text, and then click Deploy:

    ```JSON
    {
        "name": "Output Azure Blob Dataset",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService",
            "typeProperties": {
                "fileName": "StockPrices.csv",
                "folderPath": "pricedata/dfoutput",
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "external": false,
            "availability": {
                "frequency": "Minute",
                "interval": 30
            },
        "policy": {}
        }
    }
    ```

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo1\\JsonCmd2.txt**.

43. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, under Datasets, click Output Azure Blob Dataset. The JSON-formatted information is the configuration detail for the Azure storage dataset.

### Create a Data Factory pipeline with a copy activity to move the data

1. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, click More, and then click New pipeline.
2. Replace the existing JSON text with the following:

    ```JSON
    {
        "name": "Stocks DF Pipeline",
        "properties": {
            "activities": [
                {
                    "name": "SQLtoBlob",
                    "type": "Copy",
                    "inputs": [
                        {
                            "name": "Input SQL Server Dataset"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "Output Azure Blob Dataset"
                        }
                    ],
                    "policy": {
                        "timeout": "01:00:00",
                        "concurrency": 1,
                        "executionPriorityOrder": "NewestFirst",
                        "style": "StartOfInterval",
                        "retry": 0
                    },
                    "typeProperties": {
                        "source": {
                            "type": "SqlSource",
                            "sqlReaderQuery": "select * from StockPriceData"
                        },
                        "sink": {
                            "type": "BlobSink",
                            "blobWriterAddHeader": true
                        }
                    }
                }
            ],
            "start": "2017-10-23T00:00:00Z",
            "end": "2017-10-24T00:00:00Z"
        }
    }
    ```
    > **IMPORTANT**: Change the value for the start property to be yesterday's date, and the value for the end property to be tomorrow's date.

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo1\\JsonCmd3.txt**.

3. Click Deploy.
4. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, expand Pipelines, and then click Stocks DF Pipeline. The JSON-formatted information is the configuration detail for the pipeline.
5. Close the JSON editor, and stocksdf&lt;your name&gt;&lt;date&gt; blades.

### Verify the Data Factory pipeline

1. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, in the Actions section, click Diagram.
2. On the Diagram blade, double-click Input SQL Server Dataset.
3. Under Monitoring, point out the slices with a Ready status (you might need to click See more to see these, depending on your current time in relation to UTC).
4. Close the Data slices blade (if open), and then close the Input SQL Server Dataset blade.
5. On the Diagram blade, double-click Output Azure Blob Dataset.
6. Under Monitoring, point out the slices with a Ready status (you m need to click See more to see these, depending on your current time in relation to UTC).
7. Close the Data slices blade (if open), and then close the Output Azure Blob Dataset blade.
8. Close the Diagram blade.
9. On the Start menu, type Microsoft Azure Storage Explorer, and then press Enter.
10. Under your Azure Learning Pass subscription, under Storage Accounts, expand stockstore&lt;your name&gt;&lt;date&gt;, expand Blob Containers, and then click pricedata.
11. In the right pane, in the objects list, double-click dfoutput.
12. Point out the StockPrices.csv blob, which is the output from the pipeline.
13. Minimize Microsoft Azure Storage Explorer.

## Demo 2: Creating a pipeline using the Data Factory Copy Wizard

### Scenario

In this demonstration, you will see how to:

- Create a copy data activity and pipeline using the wizard.
- Verify and test the new pipeline.

### Create a copy data activity and pipeline using the wizard

1. In the Azure portal, click All resources, and then click stocksdf&lt;your name&gt;&lt;date&gt;.
2. If the Settings blade opens, close it.
3. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, in the Actions section, click Copy data (PREVIEW). This opens a new tab for Copy Data.
4. On the Copy Data page, in the Properties section, in the Task name box, type Stocks DF Copy Pipeline.
5. In the Start date time (UTC) box, click on the date, and set it to be five days before today, and then click Done.
6. In the End date time (UTC) box, click on the date, click Now to set the date to be today, and then click Done.
7. On the Copy Data page, click Next.
8. On the Copy Data page, in the Source section, click Azure Blob Storage, and then click Next.
9. On the Specify the Azure Blob storage account page, in the Connection name box, type Input Blob Storage.
10. In the Azure subscription box, select your Azure Pass subscription.
11. In the Storage account name box, click stockstore&lt;your name&gt;&lt;date&gt;, and then click Next.
12. On the Choose the input file or folder page, double-click pricedata, double-click dfoutput, double-click StockPrices.csv, and then click Next.
13. On the File format settings page, point out the PREVIEW of the data, click SCHEMA, and then scroll down. The json format is auto-detected, but in this case, you notice that PercentChange has been detected as Int64, even though it contains strings in addition to numbers.
14. At the top of the SCHEMA list, click Edit, scroll down, and change PercentChange to String.
15. Scroll up, click Save, and then click Next.
16. On the Destination data store page, click Azure SQL Database, and then click Next.
17. On the Specify the Azure SQL database page, in the Connection name box, type Output SQL Database.
18. In the Azure subscription box, select your Azure Pass subscription.
19. In the Server name box, click stockdbs&lt;your name&gt;&lt;date&gt;.
20. In the Database name box, click StockPriceDB.
21. In the User name box, type student.
22. In the Password box, type Pa55w.rd, and then click Next.
23. On the Table mapping page, in the Destination list, click [dbo].[StockData], click the down arrow, and then click SCHEMA; point out that the table structure has been read, and that all target columns have been set to type String.
24. Click Next.
25. On the Schema mapping page, point out that the data source has been mapped to the target column names, and click Next.
26. On the Settings page, click Next. The information on the scheduled pipeline in the Summary page; explain that the Copy Data wizard creates two linked services, two datasets (an input and an output), and a pipeline.
27. Click Next to deploy the pipeline.

### Verify and test the new pipeline

1. When the deployment has completed, click Click here to monitor copy pipeline. Note tThe datasets and pipeline in the diagram; there should be a Copy animation showing in the pipeline as the pipeline is active.
2. In the ACTIVITY WINDOWS list, click the most recent activity (this should still be showing as Ready) to show more details in the right-hand Activity Window Explorer pane; if you do not see this, click Refresh.
3. Scroll down through the details, and point out the data read and written, row count, copy duration, and billed duration.
4. In the ACTIVITY WINDOWS list, point out that there is an activity for each day in the period between your specified start and end times.
5. Switch to Microsoft SQL Server Management Studio.
6. In Object Explorer, expand Databases, expand StockPriceDB, expand Tables, right-click dbo.StockData, and then click Select Top 1000 Rows.

## Demo 3: Using Machine Learning in an Azure Data Factory pipeline

### Scenario

In this demonstration, you will see how to:

- Create and deploy an ML model ready for use with Data Factory.
- Upload live data as a test dataset.
- Create a Data Factory Machine Learning linked service.
- Create Data Factory input and output datasets.
- Create a new Data Factory pipeline.
- Verify and test the ML pipeline.

### Create and deploy an ML model ready for use with Data Factory

1. In the Azure portal, click All Resources, and then click StockPrices.
2. Under Additional Links, click Launch Machine Learning Studio.
3. On the Microsoft Azure Machine Learning Studio page, click Sign in here.
4. On the MY EXPERIMENTS page, click + NEW, click DATASET, and then click FROM LOCAL FILE.
5. In the Upload a new dataset dialog box, click Browse.
6. In the Choose File to Upload dialog box, go to E:\Demofiles\Mod09\Demo3, click PricesTrainingData.csv, and then click Open.
7. In the Upload a new dataset dialog box, in the SELECT A TYPE FOR THE NEW DATASET list, click Generic CSV with a header (.csv), and then click Ok (tick).
8. On the MY EXPERIMENTS page, click + NEW, click EXPERIMENT, and then click Blank Experiment.
9. In the Datasets and Modules list, expand Saved Datasets, and then expand My Datasets.
10. Drag PricesTrainingData.csv onto the workspace canvas.
11. In the Datasets and Modules list, expand Data Transformation, expand Sample and Split, and drag Split Data to the workspace canvas, below PricesTrainingData.csv.
12. Using the mouse, connect the output of PricesTrainingData.csv to the input of the Split Data module.
13. Click Split Data.
14. In the Properties pane, set Fraction of rows in the first output dataset to 0.9.
15. In the Datasets and Modules list, expand Machine Learning, expand Initialize Model, expand Regression, and then drag Neural Network Regression to the workspace canvas, to the left of the Split Data module.
16. In the Datasets and Modules list, under Machine Learning, expand Train, and drag Train Model onto the workspace canvas, below the Split Data and Neural Network Regression modules.
17. Connect the output from the Neural Network Regression module to the left input of the Train Model module.
18. Connect the left output of the Split Data module to the right input of the Train Model module.
19. Click Train Model.
20. In the Properties pane, click Launch column selector.
21. In the Select a single column dialog box, in the second list box, click column names, and in third box, click Price, and then click the check mark (tick).
22. In the Datasets and Modules list, under Machine Learning, expand Score, and drag Score Model to the workspace canvas, below and slightly to the right of the Train Model module.
23. Connect the output from the Train Model module to the left input of the Score Model module.
24. Connect the right output of the Split Data module to the right input of the Score Model module.
25. In the toolbar, click SAVE, and then click RUN.
26. When the experiment has finished running, all modules will show a green check mark to indicate that they have successfully finished.
27. When the experiment has completed, right-click the output of the Score Model module, and then click Visualize.
28. Point out that the Scored Labels column contains the predicted price made by the model, and the Price column contains the actual price.
29. Click the x at the top of the dialog box, to close the visualization.
30. In the toolbar, click SET UP WEB SERVICE, and then click Predictive Web Service [Recommended].
31. In the toolbar, click SAVE, and then click RUN (to validate and verify the changes).
32. When the experiment has finished running, ensure all modules show a green check mark to indicate1 that they have successfully finished.
33. In the toolbar, click DEPLOY WEB SERVICE, and then click Deploy Web Service [Classic].
34. Next to the API key, click the Copy button.
35. Start Notepad, type Web service API key, press Enter to create a new line, and then paste the API key.
36. Switch to Microsoft Azure Machine Learning Studio.
37. Click BATCH EXECUTION.
38. On the Batch Execution API Documentation for Experiment page, select and copy the Request URI.
39. Switch to Notepad, click at the end of the file, press Enter to create a new line, type Request URI, press Enter to create a new line, and then paste the Request URI.

### Upload live data as a test dataset

1. In Microsoft Azure Storage Explorer, under your Azure Learning Pass subscription, under Storage Accounts, under stockstore&lt;your name&gt;&lt;date&gt;, under Blob Containers, click pricedata.
2. In the right pane, in the breadcrumbs trail above the objects list, click pricedata.
3. In the toolbar, click New Folder.
4. In the Create New Virtual Directory dialog box, in the Name box, type dfinput, and then click OK.
5. In the toolbar, click Upload, and then click Upload Files.
6. In the Upload files dialog box, click the ellipsis (...).
7. In the Select files to upload dialog box, go to E:\Demofiles\Mod09\Demo3, click PricesLiveData.csv, and then click Open.
8. In the Upload files dialog box, click Upload.
9. Explain to students that you are using a simple file upload for the demo, but in a live environment, the information you have uploaded in PricesLiveData could be an output from an event hub or streaming analytics job, and delivered using another Data Factory pipeline.

### Create a Data Factory Machine Learning linked service

1. Switch to the Azure portal, click All resources, and then click stocksdf&lt;your name&gt;&lt;date&gt;.
2. On the stocksdf&lt;your name&gt;&lt;date&gt;blade, in the Actions section, click Author and deploy.
3. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, click More, click New compute, and then click Azure ML.
4. In the JSON editor, edit the following properties, and then click Deploy:
    - mlEndpoint: replace the existing value with the Request URI you copied above (remember to use double quotes around the string).
    - apiKey: replace the existing value with the Web service API key you copied above (remember to use double quotes around the key).
    - Delete the three lines marked as (Optional).
5. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, expand Linked services, and then click AzureMLLinkedService. The JSON-formatted information is the configuration detail for the connection to the Machine Learning web service, and show that the API key has now been obfuscated with *********.

### Create Data Factory input and output datasets

1. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, click More, click New dataset, and then click Azure Blob storage.
2. Replace the existing JSON text with the following, and then click Deploy:

    ```JSON
    {
        "name": "Input Blob - Decision Tree",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService",
            "typeProperties": {
                "folderPath": "pricedata/dfinput",
                "fileName": "PricesLiveData.csv",
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "external": true,
            "availability": {
                "frequency": "Minute",
                "interval": 15
            },
            "policy": {
                "externalData": {
                    "retryInterval": "00:01:00",
                    "retryTimeout": "00:10:00",
                    "maximumRetry": 3
                }
            }
        }
    }
    ```

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo3\\JsonCmd1.txt**

3. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, click More, click New dataset, and then click Azure Blob storage.
4. Replace the existing JSON text with the following, and then click Deploy:

    ```JSON
    {
        "name": "Output Blob - Decision Tree",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService",
            "typeProperties": {
                "folderPath": "pricedata/scored/{folderpart}/",
                "fileName": "{filepart}result.csv",
                "partitionedBy": [
                    {
                        "name": "folderpart",
                        "value": {
                            "type": "DateTime",
                            "date": "SliceStart",
                            "format": "yyyyMMdd"
                        }
                    },
                    {
                        "name": "filepart",
                        "value": {
                            "type": "DateTime",
                            "date": "SliceStart",
                            "format": "HHmmss"
                        }
                    }
                ],
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "availability": {
                "frequency": "Minute",
                "interval": 15
            }
        }
    }
    ```

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo3\\JsonCmd2.txt**.

### Create a new Data Factory pipeline

1. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, click More, and then click New pipeline.
2. Replace the existing JSON text with the following:

    ```JSON
    {
        "name": "ML Predictive Pipeline",
        "properties": {
            "activities": [
                {
                    "name": "ML Activity",
                    "type": "AzureMLBatchExecution",
                    "inputs": [
                        {
                            "name": "Input Blob - Decision Tree"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "Output Blob - Decision Tree"
                        }
                    ],
                    "linkedServiceName": "AzureMLLinkedService",
                    "typeProperties":
                    {
                        "webServiceInput": "Input Blob - Decision Tree",
                        "webServiceOutputs": {
                            "output1": "Output Blob - Decision Tree"
                        }
                    },
                    "policy": {
                        "concurrency": 3,
                        "executionPriorityOrder": "NewestFirst",
                        "retry": 1,
                        "timeout": "02:00:00"
                    }
                }
            ],
            "start": "2017-10-23T00:00:00Z",
            "end": "2017-10-24T00:00:00Z"
        }
     }
     ```

    > **IMPORTANT**: Change the value for the start property to be yesterday's date, and the value for the end property to be tomorrow's date.

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo3\\JsonCmd3.txt**.

3. Click Deploy.
4. Close the JSON editor, and stocksdf&lt;your name&gt;&lt;date&gt; blades.

### Verify and test the ML pipeline

1. On the stocksdf&lt;your name&gt;&lt;date&gt; blade, in the Actions section, click Diagram.
2. On the Diagram blade, use the mouse to rearrange the three pipelines so that they do not overlap.
3. Double-click Input SQL Server Dataset.
4. Under Monitoring, point out the slices with a Ready or In Progress status (you might need to click See more to see these, depending on your current time in relation to UTC).
5. Close the Data slices blade (if open), and then close the Input SQL Server Dataset blade.
6. On the Diagram blade, double-click Output Azure Blob Dataset.
7. Under Monitoring, point out the slices with a Ready or In Progress status (you might need to click See more to see these, depending on your current time in relation to UTC).
8. Close the Data slices blade (if open), and then close the Input SQL Server Dataset blade.
9. Close the Diagram blade.
10. Switch to Microsoft Azure Storage Explorer.
11. Under your Azure Learning Pass subscription, under Storage Accounts, under stockstore&lt;your name&gt;&lt;date&gt;, under Blob Containers, click pricedata.
12. In the right pane, in the breadcrumbs trail above the objects list, click pricedata.
13. In the folder list, double-click scored, and point out the dated folders.
14. Double-click one of the folders, and show the results.csv files for each timed slice.
15. Double-click one of the results.csv files.
16. In the Microsoft Azure Storage Explorer dialog box, click Yes.
17. In Excel, point out the predicted and actual prices; explain that in the real world, you would probably run the job every day, rather than every 15 minutes! Also, remind students that the output from the ML model, could itself be the input to another Data Factory pipeline.
18. Close Excel without saving any changes.
19. Close Microsoft Azure Storage Explorer.

## Demo 4: Demonstration: Using the Monitoring and Management app

### Scenario

In this demonstration, you will see how to:

- Use the Diagram view.
- Use filters.
- Pause and resume a pipeline.
- Use monitoring views.
- Use alerts.

### Use the Diagram view

1. In the Azure portal, on the stocksdf&lt;your name&gt;&lt;date&gt; blade, in the Actions section, click Monitor & Manage.
2. In the Diagram View at the top of the middle pane, use your mouse wheel or the + and - controls on the lower toolbar to zoom in and out; point out the three pipelines from the previous demos in this module.
3. In the RESOURCE EXPLORER tree view, point out the pipelines, datasets, linked services, and gateways from the previous demos.
4. In the ACTIVITY WINDOWS list at the bottom of the middle pane, point out that by default all activities are listed in reverse chronological order, with the most recent activity at the top of the list.
5. Click the top activity to show details in the Activity Window Explorer in the right pane.
6. REPHRASE THIS STEP. In the Activity Window Explorer, point out the calendar view, and show how the 15-minute data slices are represented; explain that if you used daily or weekly slices, for example, the calendar display would reflect this.
7. Scroll down and point out the summary information for this activity, including start and end time, activity name, the associated pipeline and datasets, and status information.
8. Click the cog icon to see the Properties of this activity, including more detailed execution data.
9. In the Diagram view, click Input SQL Server Dataset; point out the properties shown for this dataset in the right-hand pane.
10. Click the Script icon, to view the JSON for this dataset.
11. In the Diagram view, click Output Azure Blob Dataset; point out the properties shown for this dataset in the right-hand pane.
12. Click the Script icon, to view the JSON for this dataset.
13. In the Diagram view, click Stocks DF Pipeline.
14. In the Properties pane, click the cog icon, and point out the start and end times for the pipeline.

### Use filters

1. Click the filter for the Pipeline column.
2. In the dialog box, select the Stocks DF Pipeline check box, and then click OK.
3. Point out that using filters helps you see which activities are used by particular pipelines.
4. In the Activity Windows list, click the Copy icon.
5. In Notepad, press Ctrl+V; show that you can copy detailed activity log information for use in other applications, or for reporting.
6. Click the filter for the Status column.
7. In the dialog box, select all the check boxes except for Ready and None, and then click OK; depending on how well your previous demos performed, you might not actually see any activities at this point.
8. Click the Clear all filters icon.

### Pause and resume a pipeline

1. In the Diagram view, click ML Predictive Pipeline.
2. In the toolbar above the diagram pane, click Pause selected pipelines (the Pause button).
3. In the Pause pipelines dialog box, click OK; point out the color change to indicate the changed status.
4. To resume the pipeline, click Resume selected pipelines (the Play button).
5. In the Resume pipelines dialog box, click OK.

### Use monitoring views

1. In the left pane, click the MONITORING VIEWS icon (glasses).
2. Expand System Views, and click Failed activity windows; point out that using this view is an alternative to filtering the Activities list; again, you might not actually see any activities at this point.
3. CHECK THIS. Under System Views, click In-progress activity windows; again, depending on the state of your pipelines, you might not actually see any activities at this point. In a future update of Azure Data Factory, you will be able to create your own custom views.

### Use alerts

1. In the left pane, click the ALERTS icon (Alarm clock).
2. In the Alerts pane, click + Add Alert.
3. On the CREATE ALERT / DETAILS blade, in the Name box, type Completed activities, and then click Next.
4. On the CREATE ALERT / FILTERS blade, in the Event list, click Activity Run Finished.
5. In the Status list, click Succeeded, and then click Next.
6. On the CREATE ALERT / RECIPIENTS blade, select the Email subscription admins check box, and then click Finish.
7. In the Diagram view, right-click the Input Blob - Decision Tree activity, and then click Create alert.
8. On the CREATE ALERT / DETAILS blade, in the Name box, type Completed ML input, and then click Next.
9. On the CREATE ALERT / FILTERS blade, in the Event list, click Activity Run Started, and then click Next.
10. On the CREATE ALERT / RECIPIENTS blade, select the Email subscription admins check box, and then click Finish.
11. Open the inbox for the email address associated with your Azure subscription; steps will vary depending on the email provider.
12. You should have several alert messages; note that these might take a few minutes to arrive; point out the differences between the general alert (Completed activities), and the alert for a specific activity (Completed ML input).
13. Switch back to the Data Factory tab.
14. On the ALERTS blade, click the slider next to the Completed activities alert, to disable the alert.
15. Repeat the previous step for the Completed ML input alert.

---

Â©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.