# Module 9: Automating Data Flow with Azure Data Factory

- [Module 9: Automating Data Flow with Azure Data Factory](#module-9-automating-data-flow-with-azure-data-factory)
    - [Demo 1: Creating and running an Azure Data Factory pipeline](#demo-1-creating-and-running-an-azure-data-factory-pipeline)
        - [Scenario](#scenario)
        - [Prepare a local database for use with Azure Data Factory](#prepare-a-local-database-for-use-with-azure-data-factory)
        - [Create a new Azure Data Factory](#create-a-new-azure-data-factory)
        - [Create a Data Management Gateway](#create-a-data-management-gateway)
        - [Create Data Factory linked services for source and sink data stores](#create-data-factory-linked-services-for-source-and-sink-data-stores)
        - [Create Data Factory datasets to represent input and output data](#create-data-factory-datasets-to-represent-input-and-output-data)
        - [Create a Data Factory pipeline with a copy activity to move the data](#create-a-data-factory-pipeline-with-a-copy-activity-to-move-the-data)
        - [Verify the Data Factory pipeline](#verify-the-data-factory-pipeline)
    - [Demo 2: Creating a pipeline using the Data Factory Copy Wizard](#demo-2-creating-a-pipeline-using-the-data-factory-copy-wizard)
        - [Scenario](#scenario)
        - [Create an Azure SQL Database to act as a destination](#create-an-azure-sql-database-to-act-as-a-destination)
        - [Create a copy data activity and pipeline using the wizard](#create-a-copy-data-activity-and-pipeline-using-the-wizard)
        - [Verify and test the new pipeline](#verify-and-test-the-new-pipeline)
    - [Demo 3: Using Machine Learning in an Azure Data Factory pipeline](#demo-3-using-machine-learning-in-an-azure-data-factory-pipeline)
        - [Scenario](#scenario)
        - [Create and deploy a machine learning model ready for use with Data Factory](#create-and-deploy-a-machine-learning-model-ready-for-use-with-data-factory)
        - [Upload live data as a test dataset](#upload-live-data-as-a-test-dataset)
        - [Create a Data Factory Machine Learning linked service](#create-a-data-factory-machine-learning-linked-service)
        - [Create Data Factory input and output datasets](#create-data-factory-input-and-output-datasets)
        - [Create a new Data Factory pipeline](#create-a-new-data-factory-pipeline)
        - [Verify and test the ML pipeline](#verify-and-test-the-ml-pipeline)
    - [Demo 4: Demonstration: Using the Monitoring and Management app](#demo-4-demonstration-using-the-monitoring-and-management-app)
        - [Scenario](#scenario)
        - [Use the Diagram view to examine pipelines and datasets](#use-the-diagram-view-to-examine-pipelines-and-datasets)
        - [Use filters to examine activities](#use-filters-to-examine-activities)
        - [Pause and resume a pipeline](#pause-and-resume-a-pipeline)
        - [Use monitoring views to view the status of activities](#use-monitoring-views-to-view-the-status-of-activities)
  
## Demo 1: Creating and running an Azure Data Factory pipeline

### Scenario

In this demonstration, you will see how to use an Azure Data Factory pipeline to migrate data from an on-premises SQL Server database to Azure blob storage. Specifically, you will see how to:

- Prepare a local database for use with Azure Data Factory.
- Create a new Data Factory.
- Create a Data Management Gateway.
- Create Data Factory linked services for source and sink data stores.
- Create Data Factory datasets to represent input and output data.
- Create a Data Factory pipeline with a copy activity to move the data.
- Verify the Data Factory pipeline.

### Prepare a local database for use with Azure Data Factory

1. Ensure that the **MT17B-WS2016-NAT**, **20776A-LON-DC**, and **20776A-LON-DEV** virtual machines are running, and then log on to 20776A-LON-DEV as **ADATUM\\AdatumAdmin** with the password **Pa55w.rd**.
2. On the Windows **Start** menu, type **Microsoft SQL Server Management Studio**, and then press Enter.
3. In the **Connect to Server** dialog box, in the **Server name** box, type **LON-SQL**.
4. In the **Authentication** list, click **Windows Authentication**, and then click **Connect**.
5. In Object Explorer, expand **LON-SQL**, right-click **Databases**, and then click **New Database**.
6. In the **New Database** dialog box, in the **Database name** box, type **StockPrices**, and then click **OK**.
7. In Object Explorer, expand **Databases**, right-click **StockPrices**, and then click **New Query**.
8. In the SQL Editor, type the following commands, and then click **Execute**:

    ```SQL
    USE StockPrices
    GO

    CREATE TABLE StockPriceData
    (
        Ticker VARCHAR(4) NOT NULL,
        ClosingPrice VARCHAR(4) NOT NULL,
        OpeningPrice VARCHAR(4) NOT NULL,
        PriceChange VARCHAR(4) NOT NULL,
        PercentChange VARCHAR(30) NOT NULL,
        HourOfLastTrade VARCHAR(2) NOT NULL,
        Suspicious VARCHAR(3) NOT NULL
    )
    GO
    ```

    You can copy these commands from the file **E:\\Demofiles\\Mod09\\Demo1\\SqlCmd1.txt**

9. On the Windows **Start** menu, type **command prompt**, and then press Enter.
10. At the command prompt, run the following commands:

    ```CMD
    E:
    cd E:\Demofiles\Mod09\Demo1
    bcp StockPrices.dbo.StockPriceData in StockPriceData.csv /T /SLON-SQL /c /t,
    ```

    These commands upload sample stock price data into the **StockPricesData** table. It should read in approxiamtely 46,400 rows.

    > **Note**: Ensure that you include the comma at the end of the lst line.

    You copy these commands from the file **E:\\Demofiles\\Mod09\\Demo1\\BcpCmd1.txt**.

11. Return to Microsoft SQL Server Management Studio.
12. In the SQL Editor, replace the existing code with the following statement, and then click **Execute**:

    ```SQL
    SELECT TOP(2000) *
    FROM StockPriceData
    GO
    ```

    You can copy this statement from the file **E:\Demofiles\Mod09\Demo1\SqlCmd2.txt**. 

    This command should display the first 2,000 rows of data. The data contains stock ticker names, opening and closing prices, the price change (absolute and percentage) during the trading period, the hour of day of the last trade, and whether the trade has been marked as suspicious. As you scroll down the data, note that the PercentChange column contains integers, decimal numbers, and strings.

13. Leave Microsoft SQL Server Management Studio open.

### Create a new Azure Data Factory

1. Switch to the Azure portal.
2. Click **+ Create a resource**, click **Analytics**, and then click **Data Factory**.
3. On the **New data factory** blade, in the **Name** box, type **stocksdf&lt;your name&gt;&lt;date&gt;**.
4. Under **Resource group**, click **Create new**, and then type **StocksDF-RG**.
5. In the **Version** list, click **V1**.
6. In the **Location** list, select your nearest location from the currently available Data Factory regions, and then click **Create**.
7. Wait until the data factory has deployed before continuing with the demo.

### Create a Data Management Gateway

1. In the Azure portal, click **All resources**, and then click **stocksdf&lt;your name&gt;&lt;date&gt;**.
2. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, in the **Actions** section, click **Author and deploy**.
3. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **More**, and then click **New integration runtime (gateway)**.
4. On the **Create** blade, in the **Name** box, type **StocksGWY**, and then click **OK**.
5. On the **Configure** blade, click **Install directly on this computer**.
6. In the **Application Run - Security Warning** dialog box, click **Run**.
7. In the **User Account Control** dialog box, click **Yes**.
8. In the **Microsoft Integration Runtime Express Setup** dialog box, when the installation has completed, click **Close**.
9. If the **Message from webpage** dialog box appears, click **OK**.
10. On the Windows **Start** menu, type **Microsoft Integration Runtime**, and then press Enter.
11. In the **User Account Control** dialog box, click **Yes**.
12. In **Microsoft Integration Runtime Configuration Manager**, note that this node is connected to the cloud service.
13. Click **Diagnostics**.
14. In the **Test Connection** section, enter the following details, and then click **Test**:
    - **Data source type**: SqlServer
    - **Server**: LON-SQL
    - **Database**: StockPrices
    - **Authentication mode**: Windows
    - **User name**: ADATUM\\\\AdatumAdmin (*note the double slash*)
    - **Password**: Pa55w.rd

    Verify that the test is successful.

15. Switch to the Azure portal.
16. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, expand **Integration runtimes (Gateways)**, and then click **StocksGWY**. Note that the JSON-formatted information is the configuration detail for the gateway.

### Create Data Factory linked services for source and sink data stores

1. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **New data store**, and then click **SQL Server**.
2. In the JSON editor, edit the following properties, and then click **Deploy**:
    - **connectionString**: replace the existing value with **Data Source=LON-SQL;Initial Catalog=StockPrices;Integrated Security=True;**
    - **gatewayName**: replace the existing value with **StocksGWY**
    - **username**: replace the existing value with **ADATUM\\AdatumAdmin**
    - **password**: replace the existing value with **Pa55w.rd**
3. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, expand **Linked services**, and then click **SqlServerLinkedService**. The JSON-formatted information is the configuration detail for the connection to the on-premises SQL Server. The password has now been obfuscated with *********.
4. In the Azure portal, click **+ Create a resource**, click **Storage**, and then click **Storage account - blob, file, table,queue**.
5. On the **Create storage account** blade, by **Resource group**, click **Use existing**, and then click **StocksDF-RG**.
6. In the **Storage account name** box, type **stockstore&lt;your name&gt;&lt;date&gt;**.
7. In the **Location** list, select the same location that you used for the Data Factory.
8. In the **Account kind** list, click **Blob storage**.
9. Leave all other details at their defaults, click **Review + create**, and then click **Create**. Wait until the storage account has been successfully created before continuing with the demo.
10. Click **All resources**, and then click **stockstore&lt;your name&gt;&lt;date&gt;**.
11. On the **stockstore&lt;your name&gt;&lt;date&gt;** blade, under **Bob Service**, click **Blobs**, and then click **+ Container**.
12. In the **New container** dialog box, in the **Name** box, type **pricedata**, and then click **OK**.
13. On the **stockstore&lt;your name&gt;&lt;date&gt;** blade, under **Settings**, click **Access keys**.
14. Next to the **key1** key, click the **Click to copy** button, to copy the key to the clipboard.
15. In the Azure portal, click **All resources**, and then click **stocksdf&lt;your name&gt;&lt;date&gt;**.
16. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, in the **Actions** section, click **Author and deploy**.
17. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **New data store**, and then click **Azure Storage**.
18. In the JSON editor, in **connectionString**, replace **&lt;accountname&gt;** with **stockstore&lt;your name&gt;&lt;date&gt;**, replace **&lt;accountkey&gt;** with the storage access key you copied to the clipboard, and then click **Deploy**.
19. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, expand **Linked services**, and then click **AzureStorageLinkedService**. The JSON-formatted information is the configuration detail for the connection to the storage account, and the storage key has been obfuscated with *********.

### Create Data Factory datasets to represent input and output data

1. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **More**, click **New dataset**, and then click **SQL Server table**.
2. Replace the existing JSON text with the following, and then click **Deploy**:

    ```JSON
    {
        "name": "Input SQL Server Dataset",
        "properties": {
            "type": "SqlServerTable",
            "linkedServiceName": "SqlServerLinkedService",
            "structure": [],
            "typeProperties": {
               "tableName": "StockPriceData"
            },
            "external": true,
            "availability": {
                "frequency": "Minute",
                "interval": "30"
            },
            "policy": {
                "externalData": {
                    "retryInterval": "00:01:00",
                    "retryTimeout": "00:10:00",
                    "maximumRetry": 3
                }  
            }
        }
    }
    ```

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo1\\JsonCmd1.txt**.

3. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, expand **Datasets**, and then click **Input SQL Server Dataset**. The JSON-formatted information is the configuration detail for the on-premises SQL Server dataset.
4. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **More**, click **New dataset**, and then click **Azure Blob storage**.
5. Replace the existing JSON text with the following text, and then click **Deploy**:

    ```JSON
    {
        "name": "Output Azure Blob Dataset",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService",
            "typeProperties": {
                "fileName": "StockPrices.csv",
                "folderPath": "pricedata/dfoutput",
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "external": false,
            "availability": {
                "frequency": "Minute",
                "interval": 30
            },
        "policy": {}
        }
    }
    ```

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo1\\JsonCmd2.txt**.

6. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, under **Datasets**, click **Output Azure Blob Dataset**. The JSON-formatted information is the configuration detail for the Azure storage dataset.

### Create a Data Factory pipeline with a copy activity to move the data

1. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **More**, and then click **New pipeline**.
2. Replace the existing JSON text with the following, and then click **Deploy**.:

    ```JSON
    {
        "name": "Stocks DF Pipeline",
        "properties": {
            "activities": [
                {
                    "name": "SQLtoBlob",
                    "type": "Copy",
                    "inputs": [
                        {
                            "name": "Input SQL Server Dataset"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "Output Azure Blob Dataset"
                        }
                    ],
                    "policy": {
                        "timeout": "01:00:00",
                        "concurrency": 1,
                        "executionPriorityOrder": "NewestFirst",
                        "style": "StartOfInterval",
                        "retry": 0
                    },
                    "typeProperties": {
                        "source": {
                            "type": "SqlSource",
                            "sqlReaderQuery": "select * from StockPriceData"
                        },
                        "sink": {
                            "type": "BlobSink",
                            "blobWriterAddHeader": true
                        }
                    }
                }
            ],
            "start": "2017-10-23T00:00:00Z",
            "end": "2017-10-24T00:00:00Z"
        }
    }
    ```
    > **IMPORTANT**: Change the value for the start property to be yesterday's date, and the value for the end property to be tomorrow's date.

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo1\\JsonCmd3.txt**.

3. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, expand **Pipelines**, and then click **Stocks DF Pipeline**. The JSON-formatted information is the configuration detail for the pipeline.
4. Close the JSON editor, and the **stocksdf&lt;your name&gt;&lt;date&gt;** blades.

### Verify the Data Factory pipeline

1. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, in the **Actions** section, click **Diagram**.
2. On the **Diagram** blade, double-click **Input SQL Server Dataset**.
3. Under **Monitoring**, note the slices with a **Ready** status (you might need to click See more to see these, depending on your current time in relation to UTC).
4. Close the Data slices blade (if open), and then close the Input SQL Server Dataset blade.
5. On the Diagram blade, double-click Output Azure Blob Dataset.
6. Under Monitoring, point out the slices with a Ready status (you need to click **See more** to see these, depending on your current time in relation to UTC).
7. Close the **Data slices (by update time)** blade (if open), and then close the **Output Azure Blob Dataset** blade.
8. Close the **Diagram** blade.
9. On the desktop, on the Windows **Start** menu, type **Microsoft Azure Storage Explorer**, and then press Enter.
10. Under your Azure Learning Pass subscription, under **Storage Accounts**, expand **stockstore&lt;your name&gt;&lt;date&gt;**, expand **Blob Containers**, and then click **pricedata**.
11. In the right pane, in the objects list, double-click **dfoutput**. Note **StockPrices.csv** blob. This is the output from the pipeline.
12. Double-click **StockProces.csv** to download it and display it in Excel. In the **Microsoft Azure Storage Explorer** message box, click **Yes**.
13. Browse the data. It should be the same as that in SQL Server seen earlier.
14. Make a note of the ticker in the first row, and then close Excel.
15. Return to SQL Server Management Studio.
16. In the SQL Editor, replace the exiting command with the following. Specify the ticker that you noted in Excel in place of **&lt;Ticker&gt;**:

    ```SQL
    DELETE FROM StockPriceData
    WHERE Ticker = '<Ticker>'
    GO
    ```

17. Click **Execute**. The records for this ticker should be removed from the database. THe Data Factory pipeline should propagate this change through to the file held in blob storage.
18. Wait for a few seconds, and then switch to **Microsoft Azure Storage Explorer**.
19. Click **Refresh**, and then doubl-click **StockProces.csv** to download it. In the **Microsoft Azure Storage Explorer** message box, click **Yes**.
20. Browse the data. The records for the ticker that you deleted from SQL Server should no longer be present.
21. Close Excel, and minimize **Microsoft Azure Storage Explorer**.

## Demo 2: Creating a pipeline using the Data Factory Copy Wizard

### Scenario

In this demonstration, you will see how to use the Data Factory Copy Wizard to transfer the stock price data from Blob storage into Azure SQL Database:

- Create an Azure SQL Database to act as a destination
- Create a copy data activity and pipeline using the wizard.
- Verify and test the new pipeline.

### Create an Azure SQL Database to act as a destination

1. In the Azure portal, click **+ Create a resource**, click **Databases**, and then click **SQL Database**.
2. On the **SQL Database** blade, in the **Database name** box, type **destDB**.
3. Under **Resource group**, click **Use existing**, and then click **StocksDF-RG**.
4. In the **Select source** list, select **Blank database**.
5. Click **Server**, and then click **Create a new server**.
6. On the **New server** blade, enter the following details, and then click **Select**:
    - **Server name**: destserver&lt;your name&gt;&lt;date&gt;
    - **Server admin login**: student
    - **Password**: Pa55w.rd
    - **Confirm password**: Pa55w.rd
    - **Location**: select the same location as you used for the Data Factory
    - Leave all other settings at their default value
7. On the **SQL Database** blade, click **Pricing tier**.
8. On the **Configure** blade, click **Basic**, and then click **Apply**.
9. On the **SQL Database** blade, leave all other settings at their defaults, and then click **Create**.
10. Wait until the database and server have deployed before continuing with the demo preparation.
11. Click **All resources**, and then click **stockdbs&lt;your name&gt;&lt;date&gt;**.
12. On the **destserver&lt;your name&gt;&lt;date&gt;** blade, under **Security**, click **Firewall and virtual networks**, click **Add client IP**, click **Save**, and then click **OK**.
13. Click **All resources**, and then click **destDB (destserver&lt;your name&gt;&lt;date&gt;/destDB)**.
14. On the **destDB** blade, click **Query editor (preview)**.
15. On the **Query editor (preview)** blade, accept the default login details, and then click **OK**.
16. Click line 1, type the following command, and then click **Run**. Verify that the query runs successfully. This query creates a new table named StockNames that will hold the ticker and full stock names for each stock item:

    ```SQL
    CREATE TABLE StockData
    (
        Ticker VARCHAR(4) NOT NULL,
        ClosingPrice VARCHAR(4) NOT NULL,
        OpeningPrice VARCHAR(4) NOT NULL,
        PriceChange VARCHAR(4) NOT NULL,  
        PercentChange VARCHAR(30) NOT NULL,
        HourOfLastTrade VARCHAR(2) NOT NULL,
        Suspicious VARCHAR(3) NOT NULL
    )
    ```

    You can copy this query from the from the file **E:\\Demofiles\\Mod09\\Demo2\\SqlCmd2.txt**.


### Create a copy data activity and pipeline using the wizard

1. In the Azure portal, click **All resources**, and then click **stocksdf&lt;your name&gt;&lt;date&gt;**.
2. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, in the **Actions** section, click **Copy data (PREVIEW)**. This opens a new page in Internet Explorer.
3. On the **Copy Data (stocksdf&lt;your name&gt;&lt;date&gt;)** page, in the **Properties** section, in the **Task name** box, type **Stocks DF Copy Pipeline**.
4. In the **Start date time (UTC)** box, click the date, set it to be five days before today, and then click **Done**.
5. In the **End date time (UTC) box**, click the date, click **Now** to set the date to be today, and then click **Done**.
6. Click **Next**.
7. On the **Source data store** page, click **Azure Blob Storage**, and then click **Next**.
8. On the **Specify the Azure Blob storage account** page, in the **Connection name** box, type **Input Blob Storage**.
9. In the **Azure subscription** box, select your Azure Pass subscription.
10. In the **Storage account name** box, click **stockstore&lt;your name&gt;&lt;date&gt;**, and then click **Next**.
11. On the **Choose the input file or folder** page, double-click **pricedata**, double-click **dfoutput**, double-click **StockPrices.csv**, and then click **Next**.
12. On the **File format settings** page, in the **PREVIEW** of the data at the bottom of the page, click **SCHEMA**, and then scroll down. The json format is auto-detected, but in this case, you should observe that **PercentChange** has been detected as Int64, even though it contains strings ("NaN" and "Infinity") in addition to numbers.
13. At the top of the **SCHEMA** list, click **Edit**, scroll down, and change **PercentChange** to **String**.
14. Scroll up, click **Save**, and then click **Next**.
15. On the **Destination data store** page, click **Azure SQL Database**, and then click **Next**.
16. On the **Specify the Azure SQL database** page, in the **Connection name** box, type **Output SQL Database**.
17. In the **Azure subscription** box, select your Azure Pass subscription.
18. In the **Server name** box, click **destserver&lt;your name&gt;&lt;date&gt;**.
19. In the **Database name** box, click destDB.
20. In the **User name** box, type student.
21. In the **Password** box, type Pa55w.rd, and then click **Next**.
22. On the **Table mapping** page, in the **Destination** list, click **[dbo].[StockData]**, click the down arrow, and then click **SCHEMA**. Note out that the table structure has been read, and that all target columns have been set to type **String**.
23. Click **Next**.
24. On the **Schema mapping** page, observe that the data source has been mapped to the target column names, and then click **Next**.
25. On the **Settings** page, click **Next**. 
26. On the **Summary** page, note that the Copy Data wizard creates two linked services, two datasets (an input and an output), and a pipeline.
27. Click **Next**, and wait for the pipeline to be deployed.

### Verify and test the new pipeline

1. When the deployment has completed, click **Click here to monitor copy pipeline**. Examine the datasets and pipeline in the diagram; there should be a **Copy** animation while pipeline is active.
2. In the **ACTIVITY WINDOWS** list, click the most recent activity (this should have the status **Ready**; if you do not see an activity with this status, wait for a few seconds and then click **Refresh**) to show more details in the **Activity Window Explorer** pane.
3. Scroll down through the details, and note the amount of data read and written, row count, copy duration, and billed duration.
4. In the **ACTIVITY WINDOWS** list, notice that there is an activity for each day in the period between your specified start and end times.
5. Return to Microsoft SQL Server Management Studio.
6. In the Object Explorer toolbar, click **Connect**, and then click **Database Engine**.
7. In the **Connect to SQL Server** dialog box, in the **Server name** box type **destserver&lt;your name&gt;&lt;date&gt;.database.windows.net**, in the Authentication box click **SQL Server Authentication**, in the **Login** box type **student**, in the **Password** box type **Pa55w.rd**, and then click **Connect**.
8. In Object Explorer, expand **destserver&lt;your name&gt;&lt;date&gt;.database.windows.net**, expand **Databases**, expand **destDB**, expand **Tables**, right-click **dbo.StockData**, and then click **Select Top 1000 Rows**. You should see the first 1000 rows of stock price data transferred from Blob storage.

## Demo 3: Using Machine Learning in an Azure Data Factory pipeline

### Scenario

In this demonstration, you will see how to use Azure Data Factory to run an Azure Machine Learning model to make predictions about stock prices and capture the results. You will learn how to:

- Create and deploy a machine learning model ready for use with Data Factory.
- Upload live data as a test dataset.
- Create a Data Factory Machine Learning linked service.
- Create Data Factory input and output datasets.
- Create a new Data Factory pipeline.
- Verify and test the ML pipeline.

If you haven't performed the demos for Module 3, use the following steps to create the **StockMarket** Machine Learning workspace

1. In the Azure portal, click **+ Create a resource**.
2. In the **Search the Marketplace** box, type **Machine Learning Studio Workspace**, and then press Enter.
3. On the **Everything** blade, click **Machine Learning Studio Workspace**.
4. On the **Machine Learning Studio Workspace** blade, click **Create**.
5. On the **Machine Learning Studio workspace** blade, enter the following details, and then click **Create**:
    - **Workspace name**: StockMarket
    - **Resource group**: Use existing, and select StocksRG.
    - **Location**: select the nearest location to the one you used for the Data Lake Store.
    - **Storage account**: Create new, and name the account stocks2&lt;your_initials&gt;&lt;day number of month&gt;.
    - **Workspace pricing tier**: Standard
    - **Web service plan**: Create new, and accept the default name.
    - **Web service pricing plan tier**: S1 Standard
6. Wait until the workspace has been deployed before continuing.

### Create and deploy a machine learning model ready for use with Data Factory

1. Click **All resources**, and then click the **StockMarket** workspace.
2. On the **StockMarket Machine Learning Studio workspace** blade, under **Additional Links**, click **Launch Machine Learning Studio**.
3. On the **Microsoft Azure Machine Learning Studio** page, click **Sign in**. If prompted, sign in using your Machine Learning account credentials. If you don't already have a Machine Learning account, click **Sign up here** and follow the instructions.
4. Click **DATASETS**
5. On the **datasets** page, click **+ NEW**, click **DATASET**, and then click **FROM LOCAL FILE**.
6. In the **Upload a new dataset** dialog box, click **Choose file**.
7. In the **Open**** dialog box, go to the **E:\\Demofiles\\Mod09\\Demo3** folder, click **PricesTrainingData.csv**, and then click **Open**.
8. In the **Upload a new dataset** dialog box, in the **SELECT A TYPE FOR THE NEW DATASET** list, click **Generic CSV with a header (.csv)**, and then click **Ok** (tick).
9. Click **+ NEW**, click **EXPERIMENT**, and then click **Blank Experiment**.
10. In the **Datasets and Modules** list, expand **Saved Datasets**, and then expand **My Datasets**.
11. Drag **PricesTrainingData.csv** onto the workspace canvas.
12. In the **Datasets and Modules** list, expand **Data Transformation**, expand **Sample and Split**, and then drag the **Split Data** module to the workspace canvas, below the **PricesTrainingData.csv** dataset.
13. Using the mouse, connect the output of the **PricesTrainingData.csv** dataset to the input of the **Split Data** module.
14. Click the **Split Data** module.
15. In the **Properties** pane, set **Fraction of rows in the first output dataset** to 0.9.
16. In the **Datasets and Modules** list, expand **Machine Learning**, expand **Initialize Model**, expand **Regression**, and then drag the **Neural Network Regression** module to the workspace canvas, to the left of the **Split Data** module.
17. In the **Datasets and Modules** list, under **Machine Learning**, expand **Train**, and drag the **Train Model** module onto the workspace canvas, below the **Split Data** and **Neural Network Regression** modules.
18. Connect the output from the **Neural Network Regression** module to the left input of the **Train Model** module.
19. Connect the left output of the **Split Data** module to the right input of the **Train Model** module.
20. Click the **Train Model** module
21. In the **Properties** pane, click **Launch column selector**.
22. In the **Select a single column** dialog box, in the second list box, click **column names**, in third box, click **Price**, and then click the **Ok** (tick).
23. In the **Datasets and Modules** list, under **Machine Learning**, expand **Score**, and drag the **Score Model** module to the workspace canvas, below and slightly to the right of the **Train Model** module.
24. Connect the output from the **Train Model** module to the left input of the **Score Model** module.
25. Connect the right output of the **Split Data** module to the right input of the **Score Model** module.
26. In the toolbar, click **SAVE**, and then click **RUN**. When the experiment has finished running, all modules will show a green check mark to indicate that they have successfully finished.
27. Right-click the output of the **Score Model** module, and then click **Visualize**. The **Scored Labels** column contains the predicted price for the specified stock at the indicated time made by the model using the training data set, and the **Price** column contains the actual price of the stock at that time.
28. Close the visualization.
29. In the toolbar, click **SET UP WEB SERVICE**, and then click **Predictive Web Service [Recommended]**.
30. In the toolbar, click **SAVE**, and then click **RUN** (to validate and verify the changes).
31. When the experiment has finished running, ensure all modules show a green check mark to indicate1 that they have successfully finished.
32. In the toolbar, click **DEPLOY WEB SERVICE**, and then click **Deploy Web Service [Classic]**.
33. Copy the **API key** to the clipboard.
34. On the desktop, start Notepad, and then paste the API key into the new document.
35. Return to Microsoft Azure Machine Learning Studio.
36. Click **BATCH EXECUTION**.
37. On the **Batch Execution API Documentation for Experiment** page, select and copy the** Request URI**.
38. Switch to Notepad, and then paste the Request URI into the document on a new line.

### Upload live data as a test dataset

> **Note**: This demo uses data in a sample file, but in a live environment, the information you upload could be an output from an event hub or streaming analytics job, and delivered using another Data Factory pipeline.

1. On the desktop, in **Microsoft Azure Storage Explorer**, under your Azure Learning Pass subscription, under **Storage Accounts**, under **stockstore&lt;your name&gt;&lt;date&gt;**, under **Blob Containers**, click **pricedata**.
2. In the right pane, in the toolbar, click **+ New Folder**.
3. In the **Create New Virtual Directory** dialog box, in the **Name** box, type **dfinput**, and then click **OK**.
4. In the toolbar, click **Upload**, and then click **Upload Files**.
5. In the **Upload files** dialog box, click the ellipsis button **(...)**.
6. In the **Select files to upload** dialog box, go to the **E:\\Demofiles\\Mod09\\Demo3** folder, click **PricesLiveData.csv**, and then click **Open**.
7. In the **Upload files** dialog box, click **Upload**.

### Create a Data Factory Machine Learning linked service

1. Switch to the Azure portal, click **All resources**, and then click **stocksdf&lt;your name&gt;&lt;date&gt;**.
2. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, in the **Actions** section, click **Author and deploy**.
3. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **More**, click **New compute**, and then click **Azure ML**.
4. In the JSON editor, edit the following properties, and then click **Deploy**:
    - **mlEndpoint**: replace the existing value with the Request URI you copied to Notepad. 
    - **apiKey**: replace the existing value with the Web service API key you copied to Notepad 
    - Delete the three lines marked as (Optional).
5. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, expand **Linked services**, and then click **AzureMLLinkedService**. The JSON-formatted information is the configuration detail for the connection to the Machine Learning web service. The API key has been obfuscated with *********.

### Create Data Factory input and output datasets

1. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **More**, click **New dataset**, and then click **Azure Blob storage**.
2. Replace the existing JSON text with the following, and then click **Deploy**:

    ```JSON
    {
        "name": "Input Blob - Decision Tree",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService",
            "typeProperties": {
                "folderPath": "pricedata/dfinput",
                "fileName": "PricesLiveData.csv",
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "external": true,
            "availability": {
                "frequency": "Minute",
                "interval": 15
            },
            "policy": {
                "externalData": {
                    "retryInterval": "00:01:00",
                    "retryTimeout": "00:10:00",
                    "maximumRetry": 3
                }
            }
        }
    }
    ```

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo3\\JsonCmd1.txt**

3. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **More**, click **New dataset**, and then click **Azure Blob storage**.
4. Replace the existing JSON text with the following, and then click **Deploy**:

    ```JSON
    {
        "name": "Output Blob - Decision Tree",
        "properties": {
            "type": "AzureBlob",
            "linkedServiceName": "AzureStorageLinkedService",
            "typeProperties": {
                "folderPath": "pricedata/scored/{folderpart}/",
                "fileName": "{filepart}result.csv",
                "partitionedBy": [
                    {
                        "name": "folderpart",
                        "value": {
                            "type": "DateTime",
                            "date": "SliceStart",
                            "format": "yyyyMMdd"
                        }
                    },
                    {
                        "name": "filepart",
                        "value": {
                            "type": "DateTime",
                            "date": "SliceStart",
                            "format": "HHmmss"
                        }
                    }
                ],
                "format": {
                    "type": "TextFormat",
                    "columnDelimiter": ","
                }
            },
            "availability": {
                "frequency": "Minute",
                "interval": 15
            }
        }
    }
    ```

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo3\\JsonCmd2.txt**.

### Create a new Data Factory pipeline

1. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, click **More**, and then click **New pipeline**.
2. Replace the existing JSON text with the following, and then click **Deploy**:

    ```JSON
    {
        "name": "ML Predictive Pipeline",
        "properties": {
            "activities": [
                {
                    "name": "ML Activity",
                    "type": "AzureMLBatchExecution",
                    "inputs": [
                        {
                            "name": "Input Blob - Decision Tree"
                        }
                    ],
                    "outputs": [
                        {
                            "name": "Output Blob - Decision Tree"
                        }
                    ],
                    "linkedServiceName": "AzureMLLinkedService",
                    "typeProperties":
                    {
                        "webServiceInput": "Input Blob - Decision Tree",
                        "webServiceOutputs": {
                            "output1": "Output Blob - Decision Tree"
                        }
                    },
                    "policy": {
                        "concurrency": 3,
                        "executionPriorityOrder": "NewestFirst",
                        "retry": 1,
                        "timeout": "02:00:00"
                    }
                }
            ],
            "start": "2017-10-23T00:00:00Z",
            "end": "2017-10-24T00:00:00Z"
        }
     }
     ```

    > **IMPORTANT**: Change the value for the start property to be yesterday's date, and the value for the end property to be tomorrow's date.

    You can copy this text from the file **E:\\Demofiles\\Mod09\\Demo3\\JsonCmd3.txt**.

3. Close the JSON editor and the **stocksdf&lt;your name&gt;&lt;date&gt;** blade.

### Verify and test the ML pipeline

1. On the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, in the **Actions** section, click **Diagram**.
2. On the **Diagram** blade, use the mouse to rearrange the three pipelines so that they do not overlap.
3. Double-click **Input Blob - Decision Tree**.
4. Under **Monitoring**, note the slices with a **Ready** or **In Progress** status (you might need to click **See more** to see these, depending on your current time in relation to UTC).
5. Close the **Data slices** blade (if open), and then close the **Input Blob - Decision Tree** blade.
6. On the **Diagram** blade, double-click **Output Blob - Decision Tree**.
7. Under **Monitoring**, again note the slices with a **Ready** or **In Progress** status.
8. Close the **Output Blob - Decision Tree** blade.
9. Close the **Diagram** blade.
10. On the desktop, switch to **Microsoft Azure Storage Explorer**.
11. Under your Azure Learning Pass subscription, under **Storage Accounts**, under **stockstore&lt;your name&gt;&lt;date&gt;**, under **Blob Containers**, click **pricedata**.
12. In the right pane, in the breadcrumbs trail above the objects list, click **pricedata**.
13. In the folder list, double-click **scored**, and note the dated folders.
14. Double-click one of the folders, and then double-click the **result.csv** file for any timed slice.
15. In the **Microsoft Azure Storage Explorer** dialog box, click **Yes**.
16. In Excel, scroll through rows and compare the predicted and actual prices. In the real world, you would probably run the job every day, rather than every 15 minutes! Also, you could use the output from the ML model as the input to another Data Factory pipeline.
17. Close Excel without saving any changes.
18. Close Microsoft Azure Storage Explorer.

## Demo 4: Demonstration: Using the Monitoring and Management app

### Scenario

In this demonstration, you will see how to:

- Use the Diagram view to examine pipelines and datasets.
- Use filters. to examine activities.
- Pause and resume a pipeline.
- Use monitoring views to view the status of activities.

### Use the Diagram view to examine pipelines and datasets

1. In the Azure portal, on the **stocksdf&lt;your name&gt;&lt;date&gt;** blade, in the **Actions** section, click **Monitor & Manage**.
2. In the **Diagram** view at the top of the middle pane, use your mouse wheel or the + and - controls on the lower toolbar to zoom in and out. Examine the three pipelines from the previous demos in this module.
3. In the **ACTIVITY WINDOWS** list at the bottom of the middle pane, notice that by default all activities are listed in reverse chronological order, with the most recent activity at the top of the list.
4. Click the top activity to show details in the **Activity Window Explorer** in the right pane.
5. In the **Activity Window Explorer**, in the calendar view, note how the 15-minute data slices are represented. If you used daily or weekly slices, for example, the calendar display would reflect this.
6. Scroll down and review the summary information for this activity, including start and end time, activity name, the associated pipeline and datasets, and status information.
7. Click the cog icon to see the **Properties** pane for this activity, including more detailed execution data.
8. In the **Diagram** view, click **Input SQL Server Dataset**. Note the properties shown for this dataset in the right-hand pane.
9. Click the script icon, to view the JSON configuration for this dataset.
10. In the **Diagram** view, click **Output Azure Blob Dataset**, and then click the cog icon. The properties for this dataset appear in the right-hand pane.
11. Click the script icon, to view the JSON configuration for this dataset.
12. In the **Diagram** view, click **Stocks DF Pipeline**, and then click the cog icon. Note the start and end times, and status of the pipeline.

### Use filters to examine activities

1. In the **ACTIVITY WINDOWS** pane, click the filter for the **Pipeline** column.
2. In the dialog box, select the **Stocks DF Pipeline** check box, and then click **OK**. Using filters helps you see which activities are used by particular pipelines.
3. In the toolbar of the **ACTIVITY WINDOWS** pane, click the copy icon.
4. In Notepad, press Ctrl+V. You can copy detailed activity log information for use in other applications, or for reporting.
5. In the **ACTIVITY WINDOWS** pane, click the filter for the **Status** column.
6. In the dialog box, select all the check boxes except for **Ready** and **None**, and then click **OK**. You should see all activities that have not (yet) completed successfully.
7. Click the **Clear all filters** icon.

### Pause and resume a pipeline

1. In the **Diagram** view, click **ML Predictive Pipeline**.
2. In the toolbar above the diagram pane, click **Pause selected pipelines**.
3. In the **Pause pipelines** dialog box, click **OK**. Notice that the color in the header of the **ML Predictive Pipeline** pipeline box changes to indicate the new status.
4. To resume the pipeline, click **Resume selected pipelines** (the Play button).
5. In the **Resume pipelines** dialog box, click **OK**.

### Use monitoring views to view the status of activities

1. In the left pane, click the **MONITORING VIEWS** icon (glasses).
2. Expand **System Views**, and then click **Failed activity windows**. This view provides an alternative mechanism to filtering the Activities list.
3. Under **System Views**, click **In-progress activity windows**. You should see an activities that are currently in progress. Depending on the state of your pipelines, you might not actually see any activities at this point. 

   > **Note**: In a future update to Azure Data Factory, you will be able to create your own custom views.

---

Â©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.